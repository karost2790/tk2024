{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph self-corrective code assistant\n",
    "We wanted to test the general idea of iterative code generation in LangGraph, making a few simplifications relative to the AlphaCodium work:\n",
    "\n",
    "We start with a set of documentation specified by a user\n",
    "We use a long context LLM to ingest it, and answer a question based upon it\n",
    "We perform two layers of checking: we check imports to see if hallucinations were introduced\n",
    "We check code execution to determine if the code is able to be executed without error\n",
    "Checking for valid imports and execution is a reasonable stating point for code testing on open-ended questions related to a codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "As a test case, let's load docs related to LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\" \n",
    "loader = RecursiveUrlLoader(\n",
    "    url = url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser ( outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup( x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query ( out side the primary LCEL docs)\n",
    "url=\"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text \n",
    ")\n",
    "docs_self_query = loader.load()\n",
    "\n",
    "## Add docs\n",
    "docs.extend([*docs_pydantic, *docs_self_query])\n",
    "\n",
    "# Sort the list base on the URLs in 'metedata' -> 'source' \n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "\n",
    "# Concatednate the 'page_content' of each sorted dictionary\n",
    "concatenate_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State\n",
    "Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\" \n",
    "    Represents the state of our graph.\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "    key: Dict[str, any]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph\n",
    "Our graph lays out the logical flow shown in the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def Generate(state: GraphState):\n",
    "    \"\"\" \n",
    "    Generate a code solution base on LCEL docs and the question\n",
    "    with optional feedback from code execution tests\n",
    "\n",
    "    Args: \n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key add to state, documents, that contacts retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    ## State \n",
    "    state_dict = state[\"key\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    iter = state_dict[\"interations\"]\n",
    "\n",
    "    ## Data model\n",
    "    class code(BaseModel):\n",
    "        \"\"\"Code output\"\"\"\n",
    "\n",
    "        prefix: str = Field(description=\"Description for the problem and approach\")\n",
    "        s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crew_cli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
